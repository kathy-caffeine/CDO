\documentclass[10pt]{article}
\usepackage{graphicx} % Required for inserting images

\usepackage{polyglossia}
\setmainlanguage{russian}
\setotherlanguage{english}
\usepackage[ruled]{algorithm2e}
\SetKwComment{Comment}{}{}
\SetKw{KwIn}{Входные данные:}
\SetKw{KwOut}{Результат:}
\SetKw{KwData}{Глобальные переменные:}

\usepackage{fontspec}
\setmainfont{Times New Roman}
\newfontfamily{\cyrillicfont}{Times New Roman}

\setmonofont{Courier New}
\newfontfamily{\cyrillicfonttt}{Courier New}

\usepackage{amsmath}
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{float}
\usepackage{enumitem}
\usepackage{tikz}

\title{Chernobyl disaster optimization}
\author{Примакова Е.Е.}
\date{гр. 9301}

\begin{document}

\maketitle

\section{Введение}
Chernobyl Disaster Optimizer (CDO) - это метаэвристический алгоритм, вдохновленный поведением ядерной радиации во время Чернобыльской катастрофы. Он имитирует движение $\alpha$, $\beta$ и $\gamma$ частиц, которые распространяются от центра взрыва (области наибольшего давления) и «атакуют» человеческие цели (области наименьшего давления).
При этом алгоритм, также учитывает физические свойства частиц, такие как: скорость распространения и сособность проходить через те или иные материалы.
Каждый тип частицы ($\alpha$, $\beta$ и $\gamma$) по сути своей является уникальным поисковым агентом. 

Так $\gamma$ частицы, являясь наиболее быстрыми и "проникающими", играют роль агентов глобального поиска - наиболее быстро исследуют пространство поиска, при этом с не очень высокой точностью. $\beta$ частицы - агенты балансирующие между глобальным и локальным поиском, в то время как $\alpha$ частицы - самые медленные и с наихудшей способность проходить через материалы, являются агентвми локального поиска.

Как и в случае с реальным радиоактивным излучением, $\beta$ и $\gamma$ частицы проходят сквозь человеческое тело, в то время как $\alpha$ останавливаются. Поэтому поисковым агентом, который приходит к глобальному миимуму являются именно $\alpha$ частицы.

Также следует отметить, что поисковые агенты зависимы между собой по аналогии с PSO - все притягиваются к текущему лучшему решению, что обеспечивает сходимость и эффективное использование свойств поисковых агентов.
\section{Алгоритм и тестовые функции}

\subsection{Описание алгоритма}
Как уже говорилось, после ядерного взрыва излучение состит из $\alpha$, $\beta$ и $\gamma$ частиц. Эти частицы летят от точки взрыва (область высокого давления) до тех пор, пока не достигнут человеческих территорий (область низкого давления), где и произойдет катастрофа. Алгоритм предролагает, что жертвы (люди) идут, и эти частицы атакуют их в одно и тоже время, с разной скоростью (Табл. \ref{tab:pSpeed}).  

 \begin{table}[htbp]
     \centering
     \begin{tabular}{cc}
        Тип частицы  &  Скорость ${(s)}$\\
        \hline
          $\alpha$ & $0.16\cdot10^5$ \\
          $\beta$  & $2.7\cdot10^5$ \\
          $\gamma$  & $3\cdot10^5$
     \end{tabular}
     \caption{Скорость частиц}
     \label{tab:pSpeed}
 \end{table}

Скорость движения людей "бегущих" от радиации линейно снижается с каждой итерацией $t$, иными словами, в соответствии с (\ref{eq:whs}), на последней итерации алгоритма $T$ все умрут.

\begin{align}
    WS_h = 3 - 3\frac{t}{T},
    \label{eq:whs}
\end{align}

Изначально создаются $n$ агентов, которые представляют собой частицы трёх типов. Их начальные позиции распределяются случайным образом по всему пространству поиска. Скорости частиц ($s^{\gamma}$, $s^{\beta}$, $s^{\alpha}$) задаются в соответствии с их физическими характеристиками. Например, скорость $\gamma$-частиц является самой высокой, что делает их эффективными в глобальном поиске, тогда как $\alpha$-частицы действуют медленно, выполняя локальную оптимизацию.

Обновление позиций частиц осуществляется на каждой итерации на основе следующих формул:

\begin{align}
    v_{i,j}(t+1) &= \sum_{p \in \{\gamma, \beta, \alpha\}} \left(\frac{r \cdot A \cdot P_{i,j} - P^{p}_j}{s^p \cdot \text{rand}(0,1)} - WS_h\right), 
    \label{eq:v}\\
    P_{i,j}(t+1) &= P_{i,j}(t) + v_{i,j}(t+1),
    \label{eq:pos}
\end{align}

где $v_{i,j}$ — скорость частицы, $P_{i,j}$ — положение частицы, $A$ — радиус действия частиц, $WS_h$ — скорость жертвы, а $r$ — случайный коэффициент.

Каждая частица оценивается по значению целевой функции $f$. Если текущая пригодность частицы лучше, чем у её типа (например, у $\alpha$-частиц), то обновляются наилучшие известные значения для данного типа частиц.

Процесс повторяется до тех пор, пока не будет достигнуто максимальное количество итераций $T$ или не будет удовлетворено условие остановки. Алгоритм поддерживает равновесие между глобальным и локальным поиском благодаря сочетанию различных типов частиц и их взаимодействию через обновление скоростей.

Ниже приведены основные этапы алгоритма CDO:

\begin{enumerate}
    \item \textbf{Инициализация.} Создание агентов с случайными начальными позициями.
    \item \textbf{Обновление скоростей и позиций.} Расчёт новых скоростей и позиций с использованием физических моделей распространения частиц (\ref{eq:whs}-\ref{eq:pos}).
    \item \textbf{Оценка.} Вычисление значений целевой функции и обновление лучших значений.
    \item \textbf{Конвергенция.} Завершение работы алгоритма при достижении заданных условий.
\end{enumerate}

Основной принцип CDO заключается в использовании разных типов частиц, чтобы обеспечить эффективный баланс между исследованием пространства и уточнением решений .

Ниже приведен псевдокод рассматриваемого метода оптимизации. Входные параметры алгоритма указаны в псевдокоде. 

В пердставленном псевдокоде функция случайных чисел \texttt{rand} принимает первым аргументом размерность вектора случайных чисел, вторым аргументом - пределы в которых выбирается случайное число.
\newpage
\begin{algorithm}[H]
\KwIn{\\
$f$ \tcc*[r]{Целевая функция}
$d$ \tcc*[r]{Размерность проблемы} 
$n$ \tcc*[r]{Число частиц} 
$T$ \tcc*[r]{Максимальная итерация}
$[l, u]$ \tcc*[r]{Границы начального распределения частиц}
}
\KwOut{\\
$x_{\text{min}}$ \tcc*[r]{Точка минимума}
$f_{\text{min}}$ \tcc*[r]{Значение функции в $x_{\text{min}}$}
}
\textbf{Алгоритм:}\\
\tcc*[h]{Константы скорости распространения частиц}\\
${s}^{\gamma},~{sf}^{\gamma} \gets 3 \cdot 10^5,~1$\; 
${s}^{\alpha},~{sf}^{\alpha} \gets 0.16 \cdot 10^5,~0.25$\;
${s}^{\beta}, ~{sf}^{\beta}  \gets 2.7 \cdot 10^5,~0.5$\;
\tcc*[h]{Инициализация положений частиц в пространстве} \\
$P_i \gets \texttt{rand($d$, $[l, u]$)}$ | $i \in \left[1 \dots n\right]$\;
\BlankLine
\tcc*[h]{Основной цикл} \\
\For{$t \gets 1$ \textbf{to} $T$}{
    \For{$i \gets 1$ \textbf{to} $n$}{
        $fitness \gets f(P_i)$\;
        \uIf{$\text{fitness} < \alpha Score$}{
            $\alpha Score \gets \textit{fitness}$\;
            $P^{\alpha} \gets P_i$\;
        }
        \uElseIf{$\text{fitness} < \beta Score$}{
                $\beta Score \gets \textit{fitness}$\;
                $P^{\beta} \gets P_i$\;
        }
        \ElseIf{$\text{fitness} < \gamma Score$}{
        $\gamma Score \gets \textit{fitness}$\;
        $P^{\gamma} \gets P_i$\;
        }
    }
    ${WS}_h \gets 3 - 3(\frac{t}{T})$ \tcc*[r]{Скорость ходьбы человека}
    \BlankLine
    \tcc*[h]{Обновление положений частиц в пространстве}\\
    \For{$i \gets 1$ \textbf{to} $n$}{
        $\nu_j \gets 0~|~j\in \left[1\dots d\right]$ \tcc*[r]{Инициализация градиентов}
        \For{$j \gets 1$ \textbf{to} $d$}{
            \ForEach{$p$ \textbf{in} $\left[\gamma, \alpha,\beta\right]$}{
                $S \gets {sf}^p \cdot \log (\texttt{rand$(1$, $[1, s^p])$})$ \tcc*[r]{Скорость частицы}
                $x_h \gets \pi \cdot \texttt{rand$(1$, $[0, 1])^2$}$ \tcc*[r]{Радиус зоны ходьбы людей}
                $\rho \gets \frac{x_h}{S} - {WS}_h \cdot\texttt{rand$(1$, $[0, 1])$})$ \tcc*[r]{Распространение частицы}
                $A \gets \pi \cdot \texttt{rand$(1$, $[0, 1])^2$}$ \tcc*[r]{Радиус распространения частицы}
                $\Delta \gets \left| A \cdot P_{i,j} - P^p_j \right|$\;
                $\nu_j \gets \nu_j + P_{i,j} - \rho \cdot \Delta$ \tcc*[r]{Вектор скорости частицы}
            }
        }
        $P_i = \nu\oslash3$\;
    }
}

$f_{min}$,~$x_{min} \gets \alpha Score,~P^{\alpha}$\;
\Return{$x_{min}, f_{min}$}
\caption{Chernobyl Disaster Optimizer (CDO)}
\end{algorithm}
\subsection{Тестовые проблемы}
\subsubsection{Проблеммы с множественным локальным минимумом}
Функции этой категории характеризуются наличием множества локальных минимумов, что делает оптимизацию сложной для алгоритмов, основанных на градиентных методах, которые могут легко застрять в локальных минимумах. Эвристические и метаэвристические подходы, такие как генетические алгоритмы, оптимизация роя частиц и моделируемый отжиг, обычно лучше подходят для таких функций, поскольку они исследуют пространство решений более глобально.
\paragraph{Ackley Function}
Функция \texttt{Ackley} широко используется для тестирования алгоритмов оптимизации благодаря множеству локальных минимумов и единственному глобальному минимуму. Высокая частота колебаний мешает градиентным методам, в то время как метаэвристические алгоритмы превосходят их благодаря способности исследовать различные области пространства решений.
\begin{align}
    f(x) &= -20 \exp\left(-0.2 \sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2}\right) - \exp\left(\frac{1}{d} \sum_{i=1}^d \cos(2 \pi x_i)\right) + 20 + e,
    \label{eq:ackley}
\end{align}
\begin{align*}
    -5 \leq x_i &\leq 5, \quad x_{\text{min}} = (0, \ldots, 0), \quad f_{\text{min}} = 0.
\end{align*}

\paragraph{Eggholder Function}
Функция \texttt{Eggholder} известна своим сложным ландшафтом с многочисленными крутыми долинами и хребтами. Резкие переходы градиента делают ее сложной для квазиградиентных методов. Такие алгоритмы, как дифференциальная эволюция и роевые методы, часто оказываются более эффективными.
\begin{align}
    f(x_1, x_2) &= -(x_2 + 47) \sin\left(\sqrt{|x_2 + x_1/2 + 47|}\right) - x_1 \sin\left(\sqrt{|x_1 - (x_2 + 47)|}\right),
    \label{{eq:egg}}
\end{align}
\begin{align*}
    -512 \leq x_1, x_2 &\leq 512, \quad x_{\text{min}} \approx (512, 404.2319), \quad f_{\text{min}} \approx -959.6407.
\end{align*}

\subsubsection{Проблемы "чаши"}
Эти функции гладкие и унимодальные, с одним глобальным минимумом, который лежит в параболической области. Они хорошо подходят для градиентных и квазиньютоновских методов благодаря своей выпуклой природе.

\paragraph{Sphere Function}
\texttt{Sphere} это простой квадратичный тест, используемый для оценки алгоритмов оптимизации. Его гладкая поверхность делает его идеальным для алгоритмов, использующих информацию о градиенте.
\begin{align}
    f(x) &= \sum_{i=1}^d x_i^2,
    \label{eq:sphere}
\end{align}
\begin{align*}
    -5.12 \leq x_i &\leq 5.12, \quad x_{\text{min}} = (0, \ldots, 0), \quad f_{\text{min}} = 0.
\end{align*}

\paragraph{Trid Function}
Функция \texttt{Trid} создана для того, чтобы бросить вызов алгоритмам оптимизации с неквадратичной формой чаши. Хотя она остается унимодальной, нелинейность требует квазиньютоновских методов или продвинутых градиентных оптимизаторов для эффективного сближения.
\begin{align}
    f(x) &= \sum_{i=1}^d (x_i - 1)^2 - \sum_{i=2}^d x_i x_{i-1},
    \label{eq:trid}
\end{align}
\begin{align*}
    -d^2 \leq x_i &\leq d^2, \quad x_{\text{min}} = \left(1, 2, \ldots, d\right), \quad f_{\text{min}} = -d(d+4)(d-1)/6.
\end{align*}

\subsubsection{Функции "плато"}
Эти функции имеют плоские области или плато, что может замедлить работу алгоритмов оптимизации. Линейные методы и простой градиентный спуск часто оказываются здесь неэффективными, в то время как эвристические подходы, использующие случайную выборку или имитацию отжига, могут быть более эффективными.

\paragraph{McCormick Function}
Функция \texttt{McCormick} - это двумерная функция с относительно простой поверхностью, но невыпуклыми свойствами. Плато может ввести в заблуждение градиентные алгоритмы, но эвристические методы справляются неплохо.
\begin{align}
    f(x_1, x_2) &= \sin(x_1 + x_2) + (x_1 - x_2)^2 - 1.5x_1 + 2.5x_2 + 1,
    \label{eq:mccormick}
\end{align}
\begin{align*}
    -1.5 \leq x_1 &\leq 4, -3 \leq x_2 \leq 4, \quad x_{\text{min}} \approx (-0.54719, -1.54719), \quad f_{\text{min}} \approx -1.9133.
\end{align*}

\paragraph{Booth Function}
Функция \texttt{Booth} широко используется в оптимизации для тестирования алгоритмов с двумя переменными. Хотя функция имеет простую структуру, области плато могут замедлить сходимость для линейных или простых градиентных методо
\begin{align}
    f(x_1, x_2) &= (x_1 + 2x_2 - 7)^2 + (2x_1 + x_2 - 5)^2,
    \label{eq:booth}
\end{align}
\begin{align*}
    -10 \leq x_1, x_2 &\leq 10, \quad x_{\text{min}} = (1, 3), \quad f_{\text{min}} = 0.
\end{align*}

\subsubsection{Проблемы "долины"}
Эти функции имеют узкие долины, в которых трудно найти глобальный минимум. Градиентные алгоритмы могут колебаться в пределах долины, прежде чем сходятся. Алгоритмы с адаптивным размером шага или гибридные метаэвристические методы могут работать лучше.

\paragraph{Rosenbrock Function}
The \texttt{Rosenbrock}, также известная как функция Банана, является классическим эталоном с изогнутой долиной, ведущей к глобальному минимуму. Градиентные методы часто оказываются неэффективными из-за плоскости вдоль оси долины, что требует применения продвинутых импульсных методов.
\begin{align}
    f(x) &= \sum_{i=1}^{d-1} \left[100(x_{i+1} - x_i^2)^2 + (x_i - 1)^2\right],
    \label{eq:rosenbrock}
\end{align}
\begin{align*}
    -5 \leq x_i &\leq 10, \quad x_{\text{min}} = (1, \ldots, 1), \quad f_{\text{min}} = 0.
\end{align*}

\paragraph{Six-Hump Camel Function}
The Six-Hump Camel function is a challenging two-dimensional test case with multiple local minima. It requires algorithms with global search capabilities, such as simulated annealing or genetic algorithms, to avoid getting stuck in local minima.
\begin{align}
    f(x_1, x_2) &= 4x_1^2 - 2.1x_1^4 + \frac{x_1^6}{3} + x_1x_2 - 4x_2^2 + 4x_2^4,
    \label{eq:6camel}
\end{align}        
\begin{align*}
    -3 \leq x_1 &\leq 3, -2 \leq x_2 \leq 2, \quad x_{\text{min}} \approx (\pm 0.0898, \mp 0.7126), \quad f_{\text{min}} \approx -1.0316.
\end{align*}

\subsubsection{Проблемы "хребтов и падений"}
Функции этой категории характеризуются резкими изменениями градиента, что делает их труднопроходимыми для алгоритмов оптимизации. Метаэвристические методы, выполняющие широкую выборку, подходят лучше, чем традиционные градиентные подходы.

\paragraph{Easom Function}
Функция \texttt{Easom} имеет один резкий глобальный минимум, окруженный крутыми гребнями. Эта функция бросает вызов как градиентным, так и квазиградиентным алгоритмам, требуя стратегий глобального поиска для эффективного нахождения минимума.
\begin{align}
    f(x_1, x_2) &= -\cos(x_1) \cos(x_2) \exp\left(-(x_1 - \pi)^2 - (x_2 - \pi)^2\right), 
    \label{eq:easom}
\end{align}
\begin{align*}
    -100 \leq x_1, x_2 &\leq 100, \quad x_{\text{min}} = (\pi, \pi), \quad f_{\text{min}} = -1.   
\end{align*}


\paragraph{Michalewicz Function}
Функция \texttt{Michalewicz} - это мультимодальная проблема, зависящая от параметров $d$ (размерность) и $m$ (резкость минимумов). Крутые спады и узкие минимумы затрудняют работу большинства детерминированных методов, делая метаэвристические алгоритмы, такие как оптимизация муравьиной колонии или моделированный отжиг, более эффективными.ментальные методы.
\begin{align}
    f(x) &= -\sum_{i=1}^d \sin(x_i) \left[\sin\left(\frac{i x_i^2}{\pi}\right)\right]^{2m},
    \label{eq:mich}
\end{align}
\begin{align*}
    0 \leq x_i &\leq \pi, \quad x_{\text{min}} \text{ varies with } d, m.
\end{align*}

\section{Результаты}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1]
        \node (1) at (0,0) {\includegraphics[width=0.5\linewidth]{pict/Ackley.png}};
        \node (2) at (7,0) {\includegraphics[width=0.5\linewidth]{pict/Eggholder.png}};
        \node (3) at (-3, 2.4) {a};
        \node (3) at (4, 2.4) {b};
    \end{tikzpicture}
    \caption{Траектория поиска для проблем с множественным локальным минимумом. a - \texttt{Ackley}, b - \texttt{Eggholder}}
    \label{fig:manylocalminima}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1]
        \node (1) at (0,0) {\includegraphics[width=0.5\linewidth]{pict/Sphere.png}};
        \node (2) at (7,0) {\includegraphics[width=0.5\linewidth]{pict/Trid.png}};
        \node (3) at (-3, 2.4) {a};
        \node (3) at (4, 2.4) {b};
    \end{tikzpicture}
    \caption{Траектория поиска для проблем .... a - \texttt{Sphere}, b - \texttt{Trid}}
    \label{fig:bowl}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1]
        \node (1) at (0,0) {\includegraphics[width=0.5\linewidth]{pict/McCormick.png}};
        \node (2) at (7,0) {\includegraphics[width=0.5\linewidth]{pict/Booth.png}};
        \node (3) at (-3, 2.4) {a};
        \node (3) at (4, 2.4) {b};
    \end{tikzpicture}
    \caption{Траектория поиска для проблем .... a - \texttt{McCormick}, b - \texttt{Booth}}
    \label{fig:plate}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1]
        \node (1) at (0,0) {\includegraphics[width=0.5\linewidth]{pict/Rosenbrock.png}};
        \node (2) at (7,0) {\includegraphics[width=0.5\linewidth]{pict/SixHumpCamel.png}};
        \node (3) at (-3, 2.4) {a};
        \node (3) at (4, 2.4) {b};
    \end{tikzpicture}
    \caption{Траектория поиска для проблем.... a - \texttt{Rosenbrock}, b - \texttt{Six-Hump Camel}}
    \label{fig:valley}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1]
        \node (1) at (0,0) {\includegraphics[width=0.5\linewidth]{pict/Easom.png}};
        \node (2) at (7,0) {\includegraphics[width=0.5\linewidth]{pict/Michalewicz.png}};
        \node (3) at (-3, 2.4) {a};
        \node (3) at (4, 2.4) {b};
    \end{tikzpicture}
    \caption{Траектория поиска для проблем.... a - \texttt{Easom}, b - 
   \texttt{Michalwicz}}
    \label{fig:ridgesdrops}
\end{figure}
\section{Выводы}

Для функции Ackley, характеризующейся множеством локальных минимумов, алгоритм эффективно показал способность избегать преждевременной сходимости. Быстрое исследование пространства с помощью $\gamma$-частиц позволяет охватить всю область поиска, в то время как $\alpha$-частицы уточняют решение вблизи глобального минимума.

Функция Booth, будучи унимодальной и выпуклой, продемонстрировала быстрое достижение глобального минимума. Замедленное движение $\alpha$-частиц помогает точному нахождению решения, в то время как $\beta$- и $\gamma$-частицы обеспечивают глобальный охват.

Резкий пик функции Easom и узкий глобальный минимум создают сложность для многих оптимизаторов. $\gamma$-частицы быстро находят область, где расположен минимум, а $\alpha$-частицы успешно сужают поиск для достижения точного результата.

Сложный ландшафт функции Eggholder с крутыми гребнями и долинами демонстрирует способность оптимизатора работать в таких условиях. $\gamma$-частицы эффективно исследуют пространство, находя долины, а $\beta$-и $\alpha$-частицы постепенно уточняют решения.

Гладкость и простая нелинейность функции McCormick позволяют алгоритму быстро сходиться. Баланс между различными типами частиц приводит к быстрому достижению минимума.

Обманчивый характер функции Michalewicz, включающий узкие долины и сложный рельеф, подчёркивает преимущества алгоритма. $\gamma$-частицы предотвращают застревание в локальных минимумах, в то время как $\alpha$-частицы уточняют результаты.

Функция Rosenbrock с характерной "бананообразной" долиной представляет вызов для многих оптимизаторов. $\beta$-частицы играют важную роль в навигации по долине, тогда как $\alpha$-частицы обеспечивают точное достижение минимума.

Для функции с несколькими локальными минимумами, как Six-Hump Camel, алгоритм демонстрирует способность эффективно находить глобальный минимум благодаря сбалансированной работе $\gamma$, $\beta$ и $\alpha$-частиц.

Гладкая и выпуклая природа функции Sphere позволяет алгоритму быстро находить минимум. Сочетание различных частиц обеспечивает высокую скорость сходимости.

Функция Trid с уникальной кривизной и нелинейностью демонстрирует эффективность $\beta$-частиц в балансе между локальным и глобальным поиском, в то время как $\alpha$-частицы уточняют решение.

\begin{enumerate}
    \item \textbf{Исследование и уточнение:} Разделение ролей между $\gamma$, $\beta$ и $\alpha$-частицами позволяет эффективно балансировать между глобальным поиском и локальной оптимизацией.
    \item \textbf{Адаптивность:} Алгоритм показал гибкость при работе с разнообразными тестовыми функциями, включая мультиэкстремальные и унимодальные.
    \item \textbf{Сходимость:} Взаимодействие частиц обеспечивает надёжное достижение глобального минимума без застревания в локальных экстремумах.
\end{enumerate}

Таким образом, результаты тестирования подтверждают эффективность и универсальность оптимизатора CDO.

\end{document}
